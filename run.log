/home/deargen/Projects/06_MentalHealth_AI_Depression_JJLee/05_main_run/src/train_tft.py:114: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  dt = pd.to_datetime(df[col], errors="coerce")
/home/deargen/Projects/06_MentalHealth_AI_Depression_JJLee/05_main_run/src/train_tft.py:114: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  dt = pd.to_datetime(df[col], errors="coerce")
/home/deargen/Projects/06_MentalHealth_AI_Depression_JJLee/05_main_run/src/train_tft.py:106: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  timestamps = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)
/home/deargen/miniconda3/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/deargen/miniconda3/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.42it/s]Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.47it/s]                                                                           Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0:   0%|          | 0/68 [00:00<?, ?it/s]Epoch 0:   1%|â–         | 1/68 [00:00<00:16,  4.15it/s]Epoch 0:   1%|â–         | 1/68 [00:00<00:16,  4.15it/s, v_num=3, train_loss_step=4.260]Epoch 0:   3%|â–Ž         | 2/68 [00:00<00:09,  7.27it/s, v_num=3, train_loss_step=4.260]Epoch 0:   3%|â–Ž         | 2/68 [00:00<00:09,  7.27it/s, v_num=3, train_loss_step=4.050]Epoch 0:   4%|â–         | 3/68 [00:00<00:06,  9.59it/s, v_num=3, train_loss_step=4.050]Epoch 0:   4%|â–         | 3/68 [00:00<00:06,  9.59it/s, v_num=3, train_loss_step=3.370]Epoch 0:   6%|â–Œ         | 4/68 [00:00<00:05, 11.42it/s, v_num=3, train_loss_step=3.370]Epoch 0:   6%|â–Œ         | 4/68 [00:00<00:05, 11.41it/s, v_num=3, train_loss_step=3.930]Epoch 0:   7%|â–‹         | 5/68 [00:00<00:04, 12.77it/s, v_num=3, train_loss_step=3.930]Epoch 0:   7%|â–‹         | 5/68 [00:00<00:04, 12.76it/s, v_num=3, train_loss_step=3.900]Epoch 0:   9%|â–‰         | 6/68 [00:00<00:04, 13.80it/s, v_num=3, train_loss_step=3.900]Epoch 0:   9%|â–‰         | 6/68 [00:00<00:04, 13.79it/s, v_num=3, train_loss_step=2.790]Epoch 0:  10%|â–ˆ         | 7/68 [00:00<00:04, 14.52it/s, v_num=3, train_loss_step=2.790]Epoch 0:  10%|â–ˆ         | 7/68 [00:00<00:04, 14.51it/s, v_num=3, train_loss_step=3.660]Epoch 0:  12%|â–ˆâ–        | 8/68 [00:00<00:03, 15.22it/s, v_num=3, train_loss_step=3.660]Epoch 0:  12%|â–ˆâ–        | 8/68 [00:00<00:03, 15.22it/s, v_num=3, train_loss_step=3.000]Epoch 0:  13%|â–ˆâ–Ž        | 9/68 [00:00<00:03, 15.80it/s, v_num=3, train_loss_step=3.000]Epoch 0:  13%|â–ˆâ–Ž        | 9/68 [00:00<00:03, 15.79it/s, v_num=3, train_loss_step=4.300]Epoch 0:  15%|â–ˆâ–        | 10/68 [00:00<00:03, 16.19it/s, v_num=3, train_loss_step=4.300]Epoch 0:  15%|â–ˆâ–        | 10/68 [00:00<00:03, 16.18it/s, v_num=3, train_loss_step=3.200]Epoch 0:  16%|â–ˆâ–Œ        | 11/68 [00:00<00:03, 16.57it/s, v_num=3, train_loss_step=3.200]Epoch 0:  16%|â–ˆâ–Œ        | 11/68 [00:00<00:03, 16.56it/s, v_num=3, train_loss_step=4.570]Epoch 0:  18%|â–ˆâ–Š        | 12/68 [00:00<00:03, 16.87it/s, v_num=3, train_loss_step=4.570]Epoch 0:  18%|â–ˆâ–Š        | 12/68 [00:00<00:03, 16.87it/s, v_num=3, train_loss_step=2.650]Epoch 0:  19%|â–ˆâ–‰        | 13/68 [00:00<00:03, 17.26it/s, v_num=3, train_loss_step=2.650]Epoch 0:  19%|â–ˆâ–‰        | 13/68 [00:00<00:03, 17.26it/s, v_num=3, train_loss_step=3.140]Epoch 0:  21%|â–ˆâ–ˆ        | 14/68 [00:00<00:03, 17.56it/s, v_num=3, train_loss_step=3.140]Epoch 0:  21%|â–ˆâ–ˆ        | 14/68 [00:00<00:03, 17.55it/s, v_num=3, train_loss_step=4.780]Epoch 0:  22%|â–ˆâ–ˆâ–       | 15/68 [00:00<00:02, 17.93it/s, v_num=3, train_loss_step=4.780]Epoch 0:  22%|â–ˆâ–ˆâ–       | 15/68 [00:00<00:02, 17.93it/s, v_num=3, train_loss_step=3.160]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 16/68 [00:00<00:02, 18.08it/s, v_num=3, train_loss_step=3.160]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 16/68 [00:00<00:02, 18.08it/s, v_num=3, train_loss_step=3.080]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 17/68 [00:00<00:02, 18.05it/s, v_num=3, train_loss_step=3.080]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 17/68 [00:00<00:02, 18.05it/s, v_num=3, train_loss_step=3.790]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 18/68 [00:00<00:02, 18.25it/s, v_num=3, train_loss_step=3.790]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 18/68 [00:00<00:02, 18.24it/s, v_num=3, train_loss_step=3.990]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 19/68 [00:01<00:02, 18.52it/s, v_num=3, train_loss_step=3.990]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 19/68 [00:01<00:02, 18.52it/s, v_num=3, train_loss_step=2.870]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 20/68 [00:01<00:02, 18.73it/s, v_num=3, train_loss_step=2.870]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 20/68 [00:01<00:02, 18.72it/s, v_num=3, train_loss_step=3.940]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 21/68 [00:01<00:02, 18.83it/s, v_num=3, train_loss_step=3.940]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 21/68 [00:01<00:02, 18.83it/s, v_num=3, train_loss_step=3.100]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 22/68 [00:01<00:02, 18.92it/s, v_num=3, train_loss_step=3.100]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 22/68 [00:01<00:02, 18.91it/s, v_num=3, train_loss_step=3.630]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/68 [00:01<00:02, 19.00it/s, v_num=3, train_loss_step=3.630]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/68 [00:01<00:02, 18.99it/s, v_num=3, train_loss_step=3.720]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/68 [00:01<00:02, 19.16it/s, v_num=3, train_loss_step=3.720]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/68 [00:01<00:02, 19.16it/s, v_num=3, train_loss_step=3.350]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/68 [00:01<00:02, 19.17it/s, v_num=3, train_loss_step=3.350]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/68 [00:01<00:02, 19.17it/s, v_num=3, train_loss_step=4.730]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 26/68 [00:01<00:02, 19.26it/s, v_num=3, train_loss_step=4.730]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 26/68 [00:01<00:02, 19.25it/s, v_num=3, train_loss_step=3.050]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 27/68 [00:01<00:02, 19.36it/s, v_num=3, train_loss_step=3.050]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 27/68 [00:01<00:02, 19.36it/s, v_num=3, train_loss_step=3.390]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 28/68 [00:01<00:02, 19.45it/s, v_num=3, train_loss_step=3.390]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 28/68 [00:01<00:02, 19.45it/s, v_num=3, train_loss_step=4.320]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/68 [00:01<00:01, 19.51it/s, v_num=3, train_loss_step=4.320]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/68 [00:01<00:01, 19.51it/s, v_num=3, train_loss_step=4.420]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/68 [00:01<00:01, 19.57it/s, v_num=3, train_loss_step=4.420]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/68 [00:01<00:01, 19.57it/s, v_num=3, train_loss_step=4.190]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 31/68 [00:01<00:01, 19.68it/s, v_num=3, train_loss_step=4.190]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 31/68 [00:01<00:01, 19.68it/s, v_num=3, train_loss_step=3.360]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 32/68 [00:01<00:01, 19.74it/s, v_num=3, train_loss_step=3.360]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 32/68 [00:01<00:01, 19.74it/s, v_num=3, train_loss_step=3.850]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 33/68 [00:01<00:01, 19.74it/s, v_num=3, train_loss_step=3.850]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 33/68 [00:01<00:01, 19.74it/s, v_num=3, train_loss_step=2.820]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/68 [00:01<00:01, 19.69it/s, v_num=3, train_loss_step=2.820]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/68 [00:01<00:01, 19.68it/s, v_num=3, train_loss_step=3.210]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/68 [00:01<00:01, 19.71it/s, v_num=3, train_loss_step=3.210]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/68 [00:01<00:01, 19.71it/s, v_num=3, train_loss_step=3.130]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/68 [00:01<00:01, 19.68it/s, v_num=3, train_loss_step=3.130]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/68 [00:01<00:01, 19.68it/s, v_num=3, train_loss_step=3.640]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 37/68 [00:01<00:01, 19.71it/s, v_num=3, train_loss_step=3.640]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 37/68 [00:01<00:01, 19.71it/s, v_num=3, train_loss_step=4.360]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 38/68 [00:01<00:01, 19.73it/s, v_num=3, train_loss_step=4.360]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 38/68 [00:01<00:01, 19.73it/s, v_num=3, train_loss_step=4.070]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 39/68 [00:01<00:01, 19.75it/s, v_num=3, train_loss_step=4.070]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 39/68 [00:01<00:01, 19.75it/s, v_num=3, train_loss_step=3.550]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/68 [00:02<00:01, 19.77it/s, v_num=3, train_loss_step=3.550]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/68 [00:02<00:01, 19.77it/s, v_num=3, train_loss_step=3.130]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/68 [00:02<00:01, 19.79it/s, v_num=3, train_loss_step=3.130]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/68 [00:02<00:01, 19.78it/s, v_num=3, train_loss_step=2.970]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42/68 [00:02<00:01, 19.80it/s, v_num=3, train_loss_step=2.970]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42/68 [00:02<00:01, 19.79it/s, v_num=3, train_loss_step=3.250]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 43/68 [00:02<00:01, 19.83it/s, v_num=3, train_loss_step=3.250]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 43/68 [00:02<00:01, 19.82it/s, v_num=3, train_loss_step=2.720]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 44/68 [00:02<00:01, 19.83it/s, v_num=3, train_loss_step=2.720]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 44/68 [00:02<00:01, 19.83it/s, v_num=3, train_loss_step=3.110]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 45/68 [00:02<00:01, 19.83it/s, v_num=3, train_loss_step=3.110]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 45/68 [00:02<00:01, 19.83it/s, v_num=3, train_loss_step=3.520]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/68 [00:02<00:01, 19.82it/s, v_num=3, train_loss_step=3.520]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/68 [00:02<00:01, 19.82it/s, v_num=3, train_loss_step=4.950]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 47/68 [00:02<00:01, 19.83it/s, v_num=3, train_loss_step=4.950]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 47/68 [00:02<00:01, 19.83it/s, v_num=3, train_loss_step=4.440]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 48/68 [00:02<00:01, 19.84it/s, v_num=3, train_loss_step=4.440]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 48/68 [00:02<00:01, 19.84it/s, v_num=3, train_loss_step=3.850]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/68 [00:02<00:00, 19.85it/s, v_num=3, train_loss_step=3.850]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/68 [00:02<00:00, 19.85it/s, v_num=3, train_loss_step=3.140]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 50/68 [00:02<00:00, 19.84it/s, v_num=3, train_loss_step=3.140]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 50/68 [00:02<00:00, 19.84it/s, v_num=3, train_loss_step=3.490]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/68 [00:02<00:00, 19.84it/s, v_num=3, train_loss_step=3.490]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/68 [00:02<00:00, 19.84it/s, v_num=3, train_loss_step=4.670]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52/68 [00:02<00:00, 19.80it/s, v_num=3, train_loss_step=4.670]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52/68 [00:02<00:00, 19.80it/s, v_num=3, train_loss_step=3.290]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 53/68 [00:02<00:00, 19.79it/s, v_num=3, train_loss_step=3.290]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 53/68 [00:02<00:00, 19.79it/s, v_num=3, train_loss_step=4.650]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 54/68 [00:02<00:00, 19.84it/s, v_num=3, train_loss_step=4.650]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 54/68 [00:02<00:00, 19.84it/s, v_num=3, train_loss_step=3.480]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 55/68 [00:02<00:00, 19.87it/s, v_num=3, train_loss_step=3.480]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 55/68 [00:02<00:00, 19.87it/s, v_num=3, train_loss_step=2.700]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/68 [00:02<00:00, 19.88it/s, v_num=3, train_loss_step=2.700]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/68 [00:02<00:00, 19.88it/s, v_num=3, train_loss_step=3.970]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/68 [00:02<00:00, 19.87it/s, v_num=3, train_loss_step=3.970]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/68 [00:02<00:00, 19.87it/s, v_num=3, train_loss_step=4.340]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58/68 [00:02<00:00, 19.88it/s, v_num=3, train_loss_step=4.340]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58/68 [00:02<00:00, 19.88it/s, v_num=3, train_loss_step=3.400]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 59/68 [00:02<00:00, 19.86it/s, v_num=3, train_loss_step=3.400]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 59/68 [00:02<00:00, 19.86it/s, v_num=3, train_loss_step=4.030]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 60/68 [00:03<00:00, 19.87it/s, v_num=3, train_loss_step=4.030]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 60/68 [00:03<00:00, 19.87it/s, v_num=3, train_loss_step=3.560]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 61/68 [00:03<00:00, 19.89it/s, v_num=3, train_loss_step=3.560]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 61/68 [00:03<00:00, 19.88it/s, v_num=3, train_loss_step=3.230]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 62/68 [00:03<00:00, 19.89it/s, v_num=3, train_loss_step=3.230]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 62/68 [00:03<00:00, 19.89it/s, v_num=3, train_loss_step=3.410]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 63/68 [00:03<00:00, 19.89it/s, v_num=3, train_loss_step=3.410]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 63/68 [00:03<00:00, 19.89it/s, v_num=3, train_loss_step=3.240]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 64/68 [00:03<00:00, 19.94it/s, v_num=3, train_loss_step=3.240]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 64/68 [00:03<00:00, 19.93it/s, v_num=3, train_loss_step=3.680]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 65/68 [00:03<00:00, 19.91it/s, v_num=3, train_loss_step=3.680]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 65/68 [00:03<00:00, 19.91it/s, v_num=3, train_loss_step=3.310]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 66/68 [00:03<00:00, 19.91it/s, v_num=3, train_loss_step=3.310]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 66/68 [00:03<00:00, 19.91it/s, v_num=3, train_loss_step=4.290]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [00:03<00:00, 19.93it/s, v_num=3, train_loss_step=4.290]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [00:03<00:00, 19.92it/s, v_num=3, train_loss_step=3.500]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:03<00:00, 19.93it/s, v_num=3, train_loss_step=3.500]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:03<00:00, 19.93it/s, v_num=3, train_loss_step=3.270]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s][A
Validation DataLoader 0:   6%|â–Œ         | 1/18 [00:00<00:00, 47.57it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 2/18 [00:00<00:00, 57.90it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 3/18 [00:00<00:00, 62.83it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 4/18 [00:00<00:00, 60.48it/s][A
Validation DataLoader 0:  28%|â–ˆâ–ˆâ–Š       | 5/18 [00:00<00:00, 62.67it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:00<00:00, 64.38it/s][A
Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:00<00:00, 65.63it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:00<00:00, 66.94it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:00<00:00, 67.96it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:00<00:00, 68.46it/s][A
Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:00<00:00, 68.29it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:00<00:00, 65.35it/s][A
Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:00<00:00, 65.96it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:00<00:00, 66.65it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:00<00:00, 66.49it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:00<00:00, 66.24it/s][A
Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:00<00:00, 66.78it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 67.16it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:04<00:00, 14.07it/s, v_num=3, train_loss_step=3.270, val_loss=3.370]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:04<00:00, 14.06it/s, v_num=3, train_loss_step=3.270, val_loss=3.370, train_loss_epoch=3.630]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:04<00:00, 14.06it/s, v_num=3, train_loss_step=3.270, val_loss=3.370, train_loss_epoch=3.630]ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

Validation ROC-AUC: 0.5806
